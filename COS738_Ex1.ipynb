{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOWFR9sCI8S9uttKAEjP3Cn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dcdlima/COS738/blob/main/COS738_Ex1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sv1BfVwTLO73",
        "outputId": "f0a8eef8-2247-41a4-9ee3-973910441a99"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-07-24 11:50:51--  https://github.com/LINE-PESC/Dados-Exercicios-Texto/raw/main/Base%20para%20usar%20no%20trabalho%20CysticFibrosis2-20240401.zip\n",
            "Resolving github.com (github.com)... 140.82.113.3\n",
            "Connecting to github.com (github.com)|140.82.113.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/LINE-PESC/Dados-Exercicios-Texto/main/Base%20para%20usar%20no%20trabalho%20CysticFibrosis2-20240401.zip [following]\n",
            "--2025-07-24 11:50:51--  https://raw.githubusercontent.com/LINE-PESC/Dados-Exercicios-Texto/main/Base%20para%20usar%20no%20trabalho%20CysticFibrosis2-20240401.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1548260 (1.5M) [application/zip]\n",
            "Saving to: ‘dataset.zip’\n",
            "\n",
            "dataset.zip         100%[===================>]   1.48M  --.-KB/s    in 0.07s   \n",
            "\n",
            "2025-07-24 11:50:51 (21.9 MB/s) - ‘dataset.zip’ saved [1548260/1548260]\n",
            "\n",
            "Archive:  dataset.zip\n",
            "replace data/cf74.xml? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: data/cf74.xml           \n",
            "replace data/cf75.xml? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: data/cf75.xml           \n",
            "replace data/cf76.xml? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: data/cf76.xml           \n",
            "replace data/cf77.xml? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: data/cf77.xml           \n",
            "replace data/cf78.xml? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: data/cf78.xml           \n",
            "replace data/cf79.xml? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: data/cf79.xml           \n",
            "replace data/cfc-2.dtd? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: data/cfc-2.dtd          \n",
            "replace data/cfcquery-2.dtd? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: data/cfcquery-2.dtd     \n",
            "replace data/cfquery.xml? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: data/cfquery.xml        \n",
            "replace data/Modern Information Retrieval - Cystic Fibrosis Collection.htm? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: data/Modern Information Retrieval - Cystic Fibrosis Collection.htm  \n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ],
      "source": [
        "!wget -O dataset.zip https://github.com/LINE-PESC/Dados-Exercicios-Texto/raw/main/Base%20para%20usar%20no%20trabalho%20CysticFibrosis2-20240401.zip\n",
        "!unzip dataset.zip\n",
        "!rm -rf dataset.zip\n",
        "!rm -rf sample_data\n",
        "\n",
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "configs_dir = Path('configs')\n",
        "configs_dir.mkdir(exist_ok=True)\n",
        "\n",
        "with open(configs_dir/'PC.CFG', 'w') as conf:\n",
        "    conf.writelines('LEIA=data/cfquery.xml\\n')\n",
        "    conf.write('CONSULTAS=consultas.csv\\n')\n",
        "    conf.write('ESPERADOS=esperados.csv')\n",
        "\n",
        "with open(configs_dir/'GLI.CFG', 'w') as conf:\n",
        "    conf.write('LEIA=data/cf74.xml\\n')\n",
        "    conf.write('LEIA=data/cf75.xml\\n')\n",
        "    conf.write('LEIA=data/cf76.xml\\n')\n",
        "    conf.write('LEIA=data/cf77.xml\\n')\n",
        "    conf.write('LEIA=data/cf78.xml\\n')\n",
        "    conf.write('LEIA=data/cf79.xml\\n')\n",
        "    conf.write('ESCREVA=li.csv')\n",
        "\n",
        "with open(configs_dir/'INDEX.CFG', 'w') as conf:\n",
        "    conf.write('LEIA=li.csv\\n')\n",
        "    conf.write('ESCREVA=modelo_vetorial.json')\n",
        "\n",
        "with open(configs_dir/'BUSCA.CFG', 'w') as conf:\n",
        "    conf.write('MODELO=modelo_vetorial.json\\n')\n",
        "    conf.write('CONSULTAS=consultas.csv\\n')\n",
        "    conf.write('RESULTADOS=RESULTADOS.csv')"
      ],
      "metadata": {
        "id": "ct5NV8pzLb6i"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import xml.etree.ElementTree as ET\n",
        "import csv\n",
        "import unicodedata\n",
        "import string\n",
        "import logging\n",
        "import time\n",
        "\n",
        "# Função para normalizar texto\n",
        "def normalizar_texto(texto):\n",
        "    texto = texto.upper()\n",
        "    texto = unicodedata.normalize('NFKD', texto).encode('ASCII', 'ignore').decode('ASCII')\n",
        "    texto = texto.translate(str.maketrans('', '', string.punctuation + ';'))\n",
        "    return texto.strip()\n",
        "\n",
        "# Iniciar log\n",
        "logging.basicConfig(filename='processador_consultas.log', level=logging.INFO, format='%(asctime)s - %(message)s')\n",
        "inicio_total = time.time()\n",
        "logging.info(\"Início do Processador de Consultas\")\n",
        "\n",
        "# Ler arquivo de configuração PC.CFG\n",
        "cfg_file = 'configs/PC.CFG'\n",
        "with open(cfg_file, 'r', encoding='utf-8') as f:\n",
        "    linhas_cfg = [linha.strip() for linha in f.readlines()]\n",
        "\n",
        "arquivo_xml = linhas_cfg[0].split('=')[1]\n",
        "arquivo_consultas = linhas_cfg[1].split('=')[1]\n",
        "arquivo_esperados = linhas_cfg[2].split('=')[1]\n",
        "logging.info(f\"Arquivo de configuração lido: {cfg_file}\")\n",
        "logging.info(f\"Arquivo XML: {arquivo_xml}, Consultas: {arquivo_consultas}, Esperados: {arquivo_esperados}\")\n",
        "\n",
        "# Ler e processar o arquivo XML\n",
        "tree = ET.parse(arquivo_xml)\n",
        "root = tree.getroot()\n",
        "logging.info(\"Arquivo XML carregado\")\n",
        "\n",
        "# Gerar arquivo de consultas normalizadas\n",
        "with open(arquivo_consultas, 'w', newline='', encoding='utf-8') as f_consultas:\n",
        "    writer_consultas = csv.writer(f_consultas, delimiter=';')\n",
        "    writer_consultas.writerow(['QueryNumber', 'QueryText'])\n",
        "\n",
        "    for query in root.findall('QUERY'):\n",
        "        query_number = query.find('QueryNumber').text.strip()\n",
        "        query_text = query.find('QueryText').text.strip()\n",
        "        query_text_normalizado = normalizar_texto(query_text)\n",
        "        writer_consultas.writerow([query_number, query_text_normalizado])\n",
        "\n",
        "logging.info(\"Arquivo de consultas normalizadas gerado\")\n",
        "\n",
        "# Gerar arquivo de resultados esperados\n",
        "with open(arquivo_esperados, 'w', newline='', encoding='utf-8') as f_esperados:\n",
        "    writer_esperados = csv.writer(f_esperados, delimiter=';')\n",
        "    writer_esperados.writerow(['QueryNumber', 'DocNumber', 'DocVotes'])\n",
        "\n",
        "    for query in root.findall('QUERY'):\n",
        "        query_number = query.find('QueryNumber').text.strip()\n",
        "        for item in query.find('Records').findall('Item'):\n",
        "            score = int(item.attrib.get('Score', '0'))\n",
        "            if score > 0:\n",
        "                doc_number = item.text.strip()\n",
        "                writer_esperados.writerow([query_number, doc_number, score])\n",
        "\n",
        "logging.info(\"Arquivo de resultados esperados gerado\")\n",
        "\n",
        "fim_total = time.time()\n",
        "logging.info(f\"Processamento concluído em {fim_total - inicio_total:.2f} segundos\")\n",
        "print(\"Processador de Consultas executado com sucesso.\")\n",
        "\n",
        "\n",
        "import logging\n",
        "\n",
        "# Configuração para salvar em arquivo E mostrar no terminal\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(message)s',\n",
        "    handlers=[\n",
        "        logging.FileHandler('processador_consultas.log', mode='w', encoding='utf-8'),\n",
        "        logging.StreamHandler()\n",
        "    ]\n",
        ")\n",
        "\n",
        "!cat processador_consultas.log"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rggcvq2ZLfp3",
        "outputId": "a304d77d-1305-4057-8e21-759e2020f960"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processador de Consultas executado com sucesso.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import xml.etree.ElementTree as ET\n",
        "import csv\n",
        "import unicodedata\n",
        "import string\n",
        "import logging\n",
        "import time\n",
        "import os\n",
        "from collections import defaultdict\n",
        "\n",
        "# Função para normalizar palavras\n",
        "def normalizar_palavra(palavra):\n",
        "    palavra = palavra.upper()\n",
        "    palavra = unicodedata.normalize('NFKD', palavra).encode('ASCII', 'ignore').decode('ASCII')\n",
        "    palavra = ''.join([c for c in palavra if c.isalpha()])\n",
        "    return palavra if len(palavra) >= 1 else ''\n",
        "\n",
        "# Iniciar log\n",
        "logging.basicConfig(filename='gerador_lista_invertida.log', level=logging.INFO, format='%(asctime)s - %(message)s')\n",
        "inicio_total = time.time()\n",
        "logging.info(\"Início do Gerador de Lista Invertida\")\n",
        "\n",
        "# Ler arquivo de configuração GLI.CFG\n",
        "cfg_file = 'configs/GLI.CFG'\n",
        "with open(cfg_file, 'r', encoding='utf-8') as f:\n",
        "    linhas_cfg = [linha.strip() for linha in f.readlines()]\n",
        "\n",
        "arquivos_leia = []\n",
        "arquivo_escreva = None\n",
        "for linha in linhas_cfg:\n",
        "    if linha.startswith('LEIA='):\n",
        "        arquivos_leia.append(linha.split('=')[1])\n",
        "    elif linha.startswith('ESCREVA='):\n",
        "        arquivo_escreva = linha.split('=')[1]\n",
        "\n",
        "logging.info(f\"Arquivos XML a serem lidos: {arquivos_leia}\")\n",
        "logging.info(f\"Arquivo de saída: {arquivo_escreva}\")\n",
        "\n",
        "# Dicionário para lista invertida\n",
        "lista_invertida = defaultdict(list)\n",
        "\n",
        "# Processar cada arquivo XML\n",
        "for xml_file in arquivos_leia:\n",
        "    if not os.path.exists(xml_file):\n",
        "        logging.warning(f\"Arquivo não encontrado: {xml_file}\")\n",
        "        continue\n",
        "\n",
        "    tree = ET.parse(xml_file)\n",
        "    root = tree.getroot()\n",
        "    logging.info(f\"Processando arquivo: {xml_file}\")\n",
        "\n",
        "    for record in root.findall('RECORD'):\n",
        "        recordnum_elem = record.find('RECORDNUM')\n",
        "        if recordnum_elem is None:\n",
        "            continue\n",
        "        doc_id = int(recordnum_elem.text.strip())\n",
        "\n",
        "        abstract_elem = record.find('ABSTRACT')\n",
        "        extract_elem = record.find('EXTRACT')\n",
        "        texto = ''\n",
        "        if abstract_elem is not None and abstract_elem.text:\n",
        "            texto = abstract_elem.text\n",
        "        elif extract_elem is not None and extract_elem.text:\n",
        "            texto = extract_elem.text\n",
        "        else:\n",
        "            logging.warning(f\"Documento {doc_id} sem ABSTRACT ou EXTRACT.\")\n",
        "            continue\n",
        "\n",
        "        palavras = texto.split()\n",
        "        for palavra in palavras:\n",
        "            palavra_norm = normalizar_palavra(palavra)\n",
        "            if palavra_norm:\n",
        "                lista_invertida[palavra_norm].append(doc_id)\n",
        "\n",
        "# Gerar arquivo CSV com lista invertida\n",
        "with open(arquivo_escreva, 'w', newline='', encoding='utf-8') as f_out:\n",
        "    writer = csv.writer(f_out, delimiter=';')\n",
        "    for palavra, doc_ids in lista_invertida.items():\n",
        "        writer.writerow([palavra, doc_ids])\n",
        "\n",
        "fim_total = time.time()\n",
        "logging.info(f\"Processamento concluído em {fim_total - inicio_total:.2f} segundos\")\n",
        "print(\"Gerador de Lista Invertida executado com sucesso.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wubAivM_Nmcd",
        "outputId": "4f171598-cdcb-4e34-88dd-dce97a496927"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Documento 36 sem ABSTRACT ou EXTRACT.\n",
            "WARNING:root:Documento 128 sem ABSTRACT ou EXTRACT.\n",
            "WARNING:root:Documento 132 sem ABSTRACT ou EXTRACT.\n",
            "WARNING:root:Documento 168 sem ABSTRACT ou EXTRACT.\n",
            "WARNING:root:Documento 298 sem ABSTRACT ou EXTRACT.\n",
            "WARNING:root:Documento 329 sem ABSTRACT ou EXTRACT.\n",
            "WARNING:root:Documento 493 sem ABSTRACT ou EXTRACT.\n",
            "WARNING:root:Documento 512 sem ABSTRACT ou EXTRACT.\n",
            "WARNING:root:Documento 551 sem ABSTRACT ou EXTRACT.\n",
            "WARNING:root:Documento 595 sem ABSTRACT ou EXTRACT.\n",
            "WARNING:root:Documento 712 sem ABSTRACT ou EXTRACT.\n",
            "WARNING:root:Documento 729 sem ABSTRACT ou EXTRACT.\n",
            "WARNING:root:Documento 735 sem ABSTRACT ou EXTRACT.\n",
            "WARNING:root:Documento 839 sem ABSTRACT ou EXTRACT.\n",
            "WARNING:root:Documento 894 sem ABSTRACT ou EXTRACT.\n",
            "WARNING:root:Documento 932 sem ABSTRACT ou EXTRACT.\n",
            "WARNING:root:Documento 939 sem ABSTRACT ou EXTRACT.\n",
            "WARNING:root:Documento 1023 sem ABSTRACT ou EXTRACT.\n",
            "WARNING:root:Documento 1069 sem ABSTRACT ou EXTRACT.\n",
            "WARNING:root:Documento 1089 sem ABSTRACT ou EXTRACT.\n",
            "WARNING:root:Documento 1150 sem ABSTRACT ou EXTRACT.\n",
            "WARNING:root:Documento 1154 sem ABSTRACT ou EXTRACT.\n",
            "WARNING:root:Documento 1187 sem ABSTRACT ou EXTRACT.\n",
            "WARNING:root:Documento 1225 sem ABSTRACT ou EXTRACT.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gerador de Lista Invertida executado com sucesso.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import math\n",
        "import unicodedata\n",
        "import string\n",
        "import logging\n",
        "import time\n",
        "import pickle\n",
        "from collections import defaultdict\n",
        "\n",
        "# Função para normalizar palavras\n",
        "def normalizar_palavra(palavra):\n",
        "    palavra = palavra.upper()\n",
        "    palavra = unicodedata.normalize('NFKD', palavra).encode('ASCII', 'ignore').decode('ASCII')\n",
        "    palavra = ''.join([c for c in palavra if c.isalpha()])\n",
        "    return palavra if len(palavra) >= 2 else ''\n",
        "\n",
        "# Iniciar log\n",
        "logging.basicConfig(filename='indexador.log', level=logging.INFO, format='%(asctime)s - %(message)s')\n",
        "inicio_total = time.time()\n",
        "logging.info(\"Início do Indexador\")\n",
        "\n",
        "# Ler arquivo de configuração INDEX.CFG\n",
        "cfg_file = 'configs/INDEX.CFG'\n",
        "with open(cfg_file, 'r', encoding='utf-8') as f:\n",
        "    linhas_cfg = [linha.strip() for linha in f.readlines()]\n",
        "\n",
        "arquivo_leia = None\n",
        "arquivo_escreva = None\n",
        "for linha in linhas_cfg:\n",
        "    if linha.startswith('LEIA='):\n",
        "        arquivo_leia = linha.split('=')[1]\n",
        "    elif linha.startswith('ESCREVA='):\n",
        "        arquivo_escreva = linha.split('=')[1]\n",
        "\n",
        "logging.info(f\"Arquivo de entrada: {arquivo_leia}\")\n",
        "logging.info(f\"Arquivo de saída: {arquivo_escreva}\")\n",
        "\n",
        "# Estrutura para armazenar tf e df\n",
        "tf = defaultdict(lambda: defaultdict(int))\n",
        "df = defaultdict(int)\n",
        "documentos = set()\n",
        "\n",
        "# Ler lista invertida\n",
        "with open(arquivo_leia, 'r', encoding='utf-8') as f:\n",
        "    reader = csv.reader(f, delimiter=';')\n",
        "    for linha in reader:\n",
        "        if len(linha) != 2:\n",
        "            continue\n",
        "        termo = normalizar_palavra(linha[0])\n",
        "        if not termo:\n",
        "            continue\n",
        "        try:\n",
        "            lista_docs = eval(linha[1])\n",
        "        except:\n",
        "            continue\n",
        "        df[termo] = len(set(lista_docs))\n",
        "        for doc_id in lista_docs:\n",
        "            tf[termo][doc_id] += 1\n",
        "            documentos.add(doc_id)\n",
        "\n",
        "N = len(documentos)\n",
        "logging.info(f\"Total de documentos: {N}\")\n",
        "logging.info(f\"Total de termos distintos: {len(tf)}\")\n",
        "\n",
        "# Calcular tf-idf\n",
        "modelo_vetorial = defaultdict(dict)\n",
        "for termo, docs in tf.items():\n",
        "    idf = math.log10(N / df[termo]) if df[termo] > 0 else 0\n",
        "    for doc_id, freq in docs.items():\n",
        "        tf_normalizado = 1 + math.log10(freq) if freq > 0 else 0\n",
        "        modelo_vetorial[doc_id][termo] = tf_normalizado * idf\n",
        "\n",
        "# Salvar modelo vetorial\n",
        "with open(arquivo_escreva, 'wb') as f_out:\n",
        "    pickle.dump(dict(modelo_vetorial), f_out)\n",
        "\n",
        "fim_total = time.time()\n",
        "logging.info(f\"Indexação concluída em {fim_total - inicio_total:.2f} segundos\")\n",
        "print(\"Indexador executado com sucesso.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bv1NtfuNPEvP",
        "outputId": "c502b462-bd2f-4bcd-dc00-c2cdc93a2d61"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Indexador executado com sucesso.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import pickle\n",
        "import math\n",
        "import logging\n",
        "import time\n",
        "\n",
        "# Função para calcular distância euclidiana\n",
        "def distancia_euclidiana(vetor1, vetor2):\n",
        "    termos = set(vetor1.keys()).union(set(vetor2.keys()))\n",
        "    soma = 0\n",
        "    for termo in termos:\n",
        "        soma += (vetor1.get(termo, 0) - vetor2.get(termo, 0)) ** 2\n",
        "    return math.sqrt(soma)\n",
        "\n",
        "# Iniciar log\n",
        "logging.basicConfig(filename='buscador.log', level=logging.INFO, format='%(asctime)s - %(message)s')\n",
        "inicio_total = time.time()\n",
        "logging.info(\"Início do Buscador\")\n",
        "\n",
        "# Ler arquivo de configuração BUSCA.CFG\n",
        "cfg_file = 'configs/BUSCA.CFG'\n",
        "with open(cfg_file, 'r', encoding='utf-8') as f:\n",
        "    linhas_cfg = [linha.strip() for linha in f.readlines()]\n",
        "\n",
        "arquivo_modelo = None\n",
        "arquivo_consultas = None\n",
        "arquivo_resultados = None\n",
        "for linha in linhas_cfg:\n",
        "    if linha.startswith('MODELO='):\n",
        "        arquivo_modelo = linha.split('=')[1]\n",
        "    elif linha.startswith('CONSULTAS='):\n",
        "        arquivo_consultas = linha.split('=')[1]\n",
        "    elif linha.startswith('RESULTADOS='):\n",
        "        arquivo_resultados = linha.split('=')[1]\n",
        "\n",
        "logging.info(f\"Modelo: {arquivo_modelo}, Consultas: {arquivo_consultas}, Resultados: {arquivo_resultados}\")\n",
        "\n",
        "# Carregar modelo vetorial\n",
        "with open(arquivo_modelo, 'rb') as f_modelo:\n",
        "    modelo_vetorial = pickle.load(f_modelo)\n",
        "\n",
        "# Ler consultas\n",
        "consultas = []\n",
        "with open(arquivo_consultas, 'r', encoding='utf-8') as f_consultas:\n",
        "    reader = csv.reader(f_consultas, delimiter=';')\n",
        "    next(reader)  # pular cabeçalho\n",
        "    for linha in reader:\n",
        "        if len(linha) != 2:\n",
        "            continue\n",
        "        query_id = linha[0]\n",
        "        termos = linha[1].split()\n",
        "        vetor_consulta = {termo: 1 for termo in termos}\n",
        "        consultas.append((query_id, vetor_consulta))\n",
        "\n",
        "logging.info(f\"Total de consultas: {len(consultas)}\")\n",
        "\n",
        "# Realizar buscas\n",
        "resultados = []\n",
        "for query_id, vetor_consulta in consultas:\n",
        "    ranking = []\n",
        "    for doc_id, vetor_doc in modelo_vetorial.items():\n",
        "        distancia = distancia_euclidiana(vetor_consulta, vetor_doc)\n",
        "        ranking.append((doc_id, distancia))\n",
        "    ranking.sort(key=lambda x: x[1])  # ordenar por distância\n",
        "    lista_resultado = [(i+1, doc_id, round(distancia, 4)) for i, (doc_id, distancia) in enumerate(ranking)]\n",
        "    resultados.append((query_id, lista_resultado))\n",
        "\n",
        "# Salvar resultados\n",
        "with open(arquivo_resultados, 'w', newline='', encoding='utf-8') as f_out:\n",
        "    writer = csv.writer(f_out, delimiter=';')\n",
        "    for query_id, lista in resultados:\n",
        "        writer.writerow([query_id, lista])\n",
        "\n",
        "fim_total = time.time()\n",
        "logging.info(f\"Busca concluída em {fim_total - inicio_total:.2f} segundos\")\n",
        "print(\"Buscador executado com sucesso.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dJ_EzLAsPZu0",
        "outputId": "728b7278-e946-4136-ebd0-7952195b4641"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Buscador executado com sucesso.\n",
            "cat: buscador.log: No such file or directory\n"
          ]
        }
      ]
    }
  ]
}