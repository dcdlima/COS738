{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1HlWkp4hg07cJkC8NmXXmuktJY6q0xyaE",
      "authorship_tag": "ABX9TyP2+R8lywrIe5ipdBhhN7xn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dcdlima/COS738/blob/main/COS738_working.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sistema de Busca e Recuperaçao"
      ],
      "metadata": {
        "id": "E1Br-7KUvZLY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementação de um Sistema de Recuperação em Memória Segundo o Modelo Vetorial\n",
        "\n",
        "Este notebook apresenta a implementação completa de um sistema de recuperação da informação em memória, dividido em 4 módulos:\n",
        "\n",
        "1. **Processador de Consultas**\n",
        "2. **Gerador de Lista Invertida**\n",
        "3. **Indexador (Modelo Vetorial com TF-IDF)**\n",
        "4. **Buscador (Busca por Similaridade Cosseno)**\n",
        "\n",
        "Todos os módulos seguem o padrão proposto, utilizando arquivos de configuração, separação por etapas e logs com o módulo `logging`."
      ],
      "metadata": {
        "id": "FXcCnSjVb-LJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### O conjunto de dados\n",
        "\n",
        "O conjunto de dados foi baixado para o Google Drive e utilizado diretamente no Colab"
      ],
      "metadata": {
        "id": "RXoLLf680RpW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "# 1. Fazer upload do arquivo ZIP (se estiver no seu computador)\n",
        "uploaded = files.upload() # Descomente esta linha e execute para fazer upload\n",
        "#nome_do_arquivo_zip = list(uploaded.keys())[0]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 39
        },
        "id": "QoQ0IXii67o0",
        "outputId": "d96f6668-0b77-495f-b178-246db341e1d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-0d67a060-60fb-41bf-9504-543a3c411187\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-0d67a060-60fb-41bf-9504-543a3c411187\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Arquivos de configuraçao\n",
        "\n",
        "Foram criados a cada módulo"
      ],
      "metadata": {
        "id": "nb6RrNwgcOCo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Conjunto de dados\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aW9N-y7vgUyN",
        "outputId": "f44f45e3-bf24-4e8d-8cf6-eadefab43e5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Criaçao do arquivo CFG (configuraçao) para o processador de consultas"
      ],
      "metadata": {
        "id": "RvcjV108vgiW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def criar_pc_cfg():\n",
        "    conteudo_cfg = \"\"\"LEIA=cfquery.xml\n",
        "CONSULTAS=consultas.csv\n",
        "ESPERADOS=esperados.csv\n",
        "\"\"\"\n",
        "    with open(\"PC.CFG\", \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(conteudo_cfg)\n",
        "    print(\"Arquivo PC.CFG criado com sucesso!\")\n",
        "\n",
        "# Executa a criação\n",
        "criar_pc_cfg()"
      ],
      "metadata": {
        "id": "KEY4y-ZXjChp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import time\n",
        "import csv\n",
        "import xml.etree.ElementTree as ET\n",
        "import unicodedata\n",
        "import re\n",
        "\n",
        "# Configuração do logger\n",
        "logging.basicConfig(\n",
        "    filename='processador_consultas.log',\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "\n",
        "def remover_acentos(texto):\n",
        "    \"\"\"Remove acentos e caracteres especiais.\"\"\"\n",
        "    nfkd = unicodedata.normalize('NFKD', texto)\n",
        "    return ''.join([c for c in nfkd if not unicodedata.combining(c)])\n",
        "\n",
        "def normalizar_texto(texto):\n",
        "    \"\"\"Remove acentos, pontuações e converte para maiúsculas.\"\"\"\n",
        "    texto = remover_acentos(texto)\n",
        "    texto = re.sub(r'[^A-Za-z0-9 ]+', ' ', texto)  # Remove pontuação\n",
        "    return texto.upper().strip()\n",
        "\n",
        "def ler_configuracao(caminho_cfg=\"/content/PC.CFG\"):\n",
        "    logging.info(\"Iniciando leitura do arquivo de configuração.\")\n",
        "    config = {}\n",
        "    try:\n",
        "        with open('/content/PC.CF', \"r\", encoding=\"utf-8\") as f:\n",
        "            linhas = [linha.strip() for linha in f.readlines()]\n",
        "        chaves = [\"LEIA\", \"CONSULTAS\", \"ESPERADOS\"]\n",
        "        for i, chave in enumerate(chaves):\n",
        "            if not linhas[i].startswith(f\"{chave}=\"):\n",
        "                raise ValueError(f\"Erro no PC.CFG: linha {i+1} deveria começar com '{chave}='\")\n",
        "            config[chave] = linhas[i].split(\"=\")[1].strip()\n",
        "        logging.info(\"Leitura do arquivo de configuração concluída.\")\n",
        "        return config\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Erro ao ler PC.CFG: {e}\")\n",
        "        raise\n",
        "\n",
        "def processar_consultas(xml_path, consultas_csv, esperados_csv):\n",
        "    inicio = time.time()\n",
        "    logging.info(f\"Iniciando processamento do XML: {'/content/gdrive/MyDrive/Dados_COS738/cfquery.xml'}\")\n",
        "\n",
        "    try:\n",
        "        tree = ET.parse('/content/gdrive/MyDrive/Dados_COS738/cfquery.xml')\n",
        "        root = tree.getroot()\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Erro ao carregar XML {'/content/gdrive/MyDrive/Dados_COS738/cfquery.xml'}: {e}\")\n",
        "        raise\n",
        "\n",
        "    consultas = []\n",
        "    esperados = []\n",
        "\n",
        "    for record in root.findall(\"QUERY\"):\n",
        "        query_num = record.find(\"QueryNumber\").text.strip()\n",
        "        query_text = normalizar_texto(record.find(\"QueryText\").text)\n",
        "\n",
        "        consultas.append([query_num, query_text])\n",
        "\n",
        "        # Processa os itens esperados\n",
        "        for item in record.find(\"Records\").findall(\"Item\"):\n",
        "            doc_num = item.get(\"ref\")\n",
        "            score = int(item.get(\"score\", 0))\n",
        "            if score != 0:\n",
        "                esperados.append([query_num, doc_num, score])\n",
        "\n",
        "    # Salva consultas.csv\n",
        "    with open(consultas_csv, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
        "        writer = csv.writer(f, delimiter=\";\")\n",
        "        writer.writerow([\"QueryNumber\", \"QueryText\"])\n",
        "        writer.writerows(consultas)\n",
        "\n",
        "    # Salva esperados.csv\n",
        "    with open(esperados_csv, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
        "        writer = csv.writer(f, delimiter=\";\")\n",
        "        writer.writerow([\"QueryNumber\", \"DocNumber\", \"DocVotes\"])\n",
        "        writer.writerows(esperados)\n",
        "\n",
        "    logging.info(f\"{len(consultas)} consultas processadas.\")\n",
        "    logging.info(f\"{len(esperados)} entradas esperadas gravadas.\")\n",
        "    logging.info(f\"Tempo total do processamento: {time.time() - inicio:.2f} segundos.\")\n",
        "\n",
        "def main():\n",
        "    logging.info(\"===== Início do módulo Processador de Consultas =====\")\n",
        "    tempo_total = time.time()\n",
        "    try:\n",
        "        config = ler_configuracao(\"PC.CFG\")\n",
        "        processar_consultas(config[\"LEIA\"], config[\"CONSULTAS\"], config[\"ESPERADOS\"])\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Erro no módulo Processador de Consultas: {e}\")\n",
        "    logging.info(f\"===== Fim do módulo (tempo total: {time.time() - tempo_total:.2f} seg) =====\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "with open(\"processador_consultas.log\", \"r\", encoding=\"utf-8\") as f:\n",
        "    print(f.read())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lJbc7otk3K7S",
        "outputId": "edaa196a-0629-466c-dc48-776c5be72b0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-29 10:29:30,391 - INFO - ===== Início do módulo Processador de Consultas =====\n",
            "2025-07-29 10:29:30,391 - INFO - Iniciando leitura do arquivo de configuração.\n",
            "2025-07-29 10:29:30,391 - INFO - Leitura do arquivo de configuração concluída.\n",
            "2025-07-29 10:29:30,391 - INFO - Iniciando processamento do XML: /content/drive/MyDrive/Dados_COS738/cfquery.xml\n",
            "2025-07-29 10:29:30,391 - ERROR - Erro no módulo Processador de Consultas: [Errno 2] No such file or directory: 'cfquery.xml'\n",
            "2025-07-29 10:29:30,391 - INFO - ===== Fim do módulo (tempo total: 0.00 seg) =====\n",
            "2025-07-29 10:30:06,487 - INFO - NumExpr defaulting to 2 threads.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def criar_gli_cfg():\n",
        "    arquivos = [\"/content/cf74.xml\", \"/content/cf75.xml\", \"/content/cf76.xml\", \"/content/cf77.xml\", \"/content/cf78.xml\", \"/content/cf79.xml\"]\n",
        "    caminho_saida = \"lista_invertida.csv\"\n",
        "\n",
        "    with open(\"GLI.CFG\", \"w\", encoding=\"utf-8\") as f:\n",
        "        for nome in arquivos:\n",
        "            f.write(f\"LEIA={nome}\\n\")\n",
        "        f.write(f\"ESCREVA={caminho_saida}\\n\")\n",
        "\n",
        "    print(\"Arquivo GLI.CFG criado com sucesso!\")\n",
        "\n",
        "# Executar\n",
        "criar_gli_cfg()"
      ],
      "metadata": {
        "id": "_wdiS7GCxjL3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import time\n",
        "import unicodedata\n",
        "import re\n",
        "import xml.etree.ElementTree as ET\n",
        "import csv\n",
        "from collections import defaultdict\n",
        "\n",
        "# Reconfigura o logger, mesmo se já estiver configurado\n",
        "for handler in logging.root.handlers[:]:\n",
        "    logging.root.removeHandler(handler)\n",
        "\n",
        "logging.basicConfig(\n",
        "    filename='gerador_lista_invertida.log',\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "\n",
        "def remover_acentos(texto):\n",
        "    nfkd = unicodedata.normalize('NFKD', texto)\n",
        "    return ''.join([c for c in nfkd if not unicodedata.combining(c)])\n",
        "\n",
        "def normalizar_texto(texto):\n",
        "    texto = remover_acentos(texto)\n",
        "    texto = re.sub(r'[^A-Za-z ]+', ' ', texto)\n",
        "    return texto.upper().strip()\n",
        "\n",
        "def ler_configuracao(gli_cfg=\"GLI.CFG\"):\n",
        "    logging.info(\"Lendo arquivo de configuração GLI.CFG\")\n",
        "    arquivos_leitura = []\n",
        "    arquivo_saida = None\n",
        "    with open('/content/GLI.CFG', \"r\", encoding=\"utf-8\") as f:\n",
        "        linhas = [linha.strip() for linha in f.readlines()]\n",
        "    for linha in linhas:\n",
        "        if linha.startswith(\"LEIA=\"):\n",
        "            arquivos_leitura.append(linha.split(\"=\")[1].strip())\n",
        "        elif linha.startswith(\"ESCREVA=\"):\n",
        "            if arquivo_saida:\n",
        "                raise ValueError(\"Mais de um ESCREVA no arquivo de configuração.\")\n",
        "            arquivo_saida = linha.split(\"=\")[1].strip()\n",
        "    if not arquivos_leitura or not arquivo_saida:\n",
        "        raise ValueError(\"Faltam instruções LEIA ou ESCREVA no GLI.CFG.\")\n",
        "    logging.info(f\"{len(arquivos_leitura)} arquivos para leitura. Saída: {arquivo_saida}\")\n",
        "    return arquivos_leitura, arquivo_saida\n",
        "\n",
        "def processar_documentos(arquivos_xml):\n",
        "    lista_invertida = defaultdict(list)\n",
        "    total_docs = 0\n",
        "    total_palavras = 0\n",
        "    for arquivo in arquivos_xml:\n",
        "        logging.info(f\"Lendo arquivo {arquivo}\")\n",
        "        try:\n",
        "            tree = ET.parse(arquivo)\n",
        "            root = tree.getroot()\n",
        "            for record in root.findall(\"RECORD\"):\n",
        "                recordnum_elem = record.find(\"RECORDNUM\")\n",
        "                if recordnum_elem is None:\n",
        "                    continue\n",
        "                doc_id = int(recordnum_elem.text.strip())\n",
        "                texto_elem = record.find(\"ABSTRACT\") or record.find(\"EXTRACT\")\n",
        "                if texto_elem is None:\n",
        "                    logging.warning(f\"Documento {doc_id} sem ABSTRACT ou EXTRACT.\")\n",
        "                    continue\n",
        "                texto = normalizar_texto(texto_elem.text or \"\")\n",
        "                palavras = texto.split()\n",
        "                palavras_validas = [p for p in palavras if len(p) >= 2 and p.isalpha()]\n",
        "                for palavra in palavras_validas:\n",
        "                    lista_invertida[palavra].append(doc_id)\n",
        "                total_docs += 1\n",
        "                total_palavras += len(palavras_validas)\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Erro ao processar {arquivo}: {e}\")\n",
        "    return lista_invertida, total_docs, total_palavras\n",
        "\n",
        "def salvar_lista_invertida(lista_invertida, caminho_saida):\n",
        "    logging.info(f\"Salvando lista invertida em {caminho_saida}\")\n",
        "    with open(caminho_saida, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
        "        writer = csv.writer(f, delimiter=\";\")\n",
        "        writer.writerow([\"Palavra\", \"Documentos\"])\n",
        "        for palavra, docs in sorted(lista_invertida.items()):\n",
        "            writer.writerow([palavra, str(docs)])\n",
        "\n",
        "def main():\n",
        "    logging.info(\"===== Início do módulo Gerador de Lista Invertida =====\")\n",
        "    inicio_total = time.time()\n",
        "    try:\n",
        "        arquivos_xml, saida_csv = ler_configuracao()\n",
        "        lista_invertida, total_docs, total_palavras = processar_documentos(arquivos_xml)\n",
        "        salvar_lista_invertida(lista_invertida, saida_csv)\n",
        "        tempo_total = time.time() - inicio_total\n",
        "        logging.info(f\"Total de documentos processados: {total_docs}\")\n",
        "        logging.info(f\"Total de palavras indexadas: {total_palavras}\")\n",
        "        logging.info(f\"Total de termos únicos: {len(lista_invertida)}\")\n",
        "        logging.info(f\"Tempo total: {tempo_total:.2f} segundos\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Erro geral: {e}\")\n",
        "    logging.info(\"===== Fim do módulo Gerador de Lista Invertida =====\")\n",
        "\n",
        "# Executar o módulo\n",
        "main()\n",
        "\n"
      ],
      "metadata": {
        "id": "wXWNc6fIxmYp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"gerador_lista_invertida.log\", \"r\", encoding=\"utf-8\") as f:\n",
        "    print(f.read())"
      ],
      "metadata": {
        "id": "EwcMTRHNyM0u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualizando o arquivo csv gerado parcialmente (até o item 10)"
      ],
      "metadata": {
        "id": "AxOsDZg9L_pl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_lista = pd.read_csv(\"lista_invertida.csv\", sep=\";\")\n",
        "df_lista.head(10)"
      ],
      "metadata": {
        "id": "fJemSSDEL5iZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Módulo 3 - Indexador"
      ],
      "metadata": {
        "id": "2VwYuuHsPbmt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Criar arquivo de configuraçao do indexador"
      ],
      "metadata": {
        "id": "1bcsdhVsPgDe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def criar_index_cfg():\n",
        "    with open(\"INDEX.CFG\", \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(\"LEIA=lista_invertida.csv\\n\")\n",
        "        f.write(\"ESCREVA=modelo_vetorial.pkl\\n\")\n",
        "    print(\"Arquivo INDEX.CFG criado com sucesso!\")\n",
        "\n",
        "# Executar\n",
        "criar_index_cfg()"
      ],
      "metadata": {
        "id": "QyKgl26aPe5k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import time\n",
        "import csv\n",
        "import math\n",
        "import pickle\n",
        "from collections import defaultdict\n",
        "\n",
        "# Reconfigura logger se necessário\n",
        "for handler in logging.root.handlers[:]:\n",
        "    logging.root.removeHandler(handler)\n",
        "\n",
        "logging.basicConfig(\n",
        "    filename='indexador.log',\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "\n",
        "def ler_configuracao_indexador(caminho_cfg=\"INDEX.CFG\"):\n",
        "    logging.info(\"Lendo arquivo de configuração INDEX.CFG\")\n",
        "    with open('/content/INDEX.CFG', \"r\", encoding=\"utf-8\") as f:\n",
        "        linhas = [linha.strip() for linha in f.readlines()]\n",
        "    leia = None\n",
        "    escreva = None\n",
        "    for linha in linhas:\n",
        "        if linha.startswith(\"LEIA=\"):\n",
        "            leia = linha.split(\"=\")[1].strip()\n",
        "        elif linha.startswith(\"ESCREVA=\"):\n",
        "            escreva = linha.split(\"=\")[1].strip()\n",
        "    if not leia or not escreva:\n",
        "        raise ValueError(\"Arquivo de configuração INDEX.CFG inválido.\")\n",
        "    return leia, escreva\n",
        "\n",
        "def carregar_lista_invertida(caminho_csv):\n",
        "    logging.info(f\"Carregando lista invertida de {'/content/lista_invertida.csv'}\")\n",
        "    lista_invertida = {}\n",
        "    with open('/content/lista_invertida.csv', \"r\", encoding=\"utf-8\") as f:\n",
        "        reader = csv.reader(f, delimiter=\";\")\n",
        "        next(reader)  # pula cabeçalho\n",
        "        for linha in reader:\n",
        "            termo = linha[0]\n",
        "            docs_str = linha[1]\n",
        "            docs = eval(docs_str)  # converte a lista a partir da string\n",
        "            lista_invertida[termo] = docs\n",
        "    return lista_invertida\n",
        "\n",
        "def calcular_tfidf(lista_invertida):\n",
        "    logging.info(\"Calculando TF-IDF\")\n",
        "    modelo = defaultdict(dict)\n",
        "    total_docs = set()\n",
        "\n",
        "    # Passo 1: contar DF (quantos docs cada termo aparece)\n",
        "    df = {termo: len(set(docs)) for termo, docs in lista_invertida.items()}\n",
        "\n",
        "    # Passo 2: identificar todos os documentos\n",
        "    for docs in lista_invertida.values():\n",
        "        total_docs.update(docs)\n",
        "    N = len(total_docs)\n",
        "\n",
        "    # Passo 3: calcular TF-IDF para cada termo/doc\n",
        "    for termo, docs in lista_invertida.items():\n",
        "        tf_counts = defaultdict(int)\n",
        "        for doc_id in docs:\n",
        "            tf_counts[doc_id] += 1\n",
        "\n",
        "        for doc_id, tf in tf_counts.items():\n",
        "            idf = math.log(N / df[termo]) if df[termo] != 0 else 0\n",
        "            peso = tf * idf\n",
        "            modelo[doc_id][termo] = peso\n",
        "\n",
        "    return dict(modelo)\n",
        "\n",
        "def salvar_modelo(modelo, caminho_saida):\n",
        "    logging.info(f\"Salvando modelo vetorial em {caminho_saida}\")\n",
        "    with open(caminho_saida, \"wb\") as f:\n",
        "        pickle.dump(modelo, f)\n",
        "\n",
        "def main():\n",
        "    logging.info(\"===== Início do módulo Indexador =====\")\n",
        "    inicio = time.time()\n",
        "    try:\n",
        "        entrada_csv, saida_modelo = ler_configuracao_indexador()\n",
        "        lista_invertida = carregar_lista_invertida(entrada_csv)\n",
        "        modelo = calcular_tfidf(lista_invertida)\n",
        "        salvar_modelo(modelo, saida_modelo)\n",
        "        logging.info(f\"Modelo criado com {len(modelo)} documentos.\")\n",
        "        logging.info(f\"Tempo total: {time.time() - inicio:.2f} segundos.\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Erro no indexador: {e}\")\n",
        "    logging.info(\"===== Fim do módulo Indexador =====\")\n",
        "\n",
        "# Executar o módulo\n",
        "main()\n",
        "\n",
        "with open(\"indexador.log\", \"r\", encoding=\"utf-8\") as f:\n",
        "    print (f.read())\n"
      ],
      "metadata": {
        "id": "1GUyCgRLPvV4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Módulo 4 - Buscador"
      ],
      "metadata": {
        "id": "AoKWtgBeTr9B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Criaçao do arquivo de configuraçao do buscador"
      ],
      "metadata": {
        "id": "9wKJqCBATv0q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def criar_busca_cfg():\n",
        "    with open(\"BUSCA.CFG\", \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(\"MODELO=modelo_vetorial.pkl\\n\")\n",
        "        f.write(\"CONSULTAS=consultas.csv\\n\")\n",
        "        f.write(\"RESULTADOS=resultados.csv\\n\")\n",
        "    print(\"Arquivo BUSCA.CFG criado com sucesso!\")\n",
        "\n",
        "criar_busca_cfg()"
      ],
      "metadata": {
        "id": "cQRALsDQT1fF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Buscador pelo princípio de similaridade de cosseno"
      ],
      "metadata": {
        "id": "388iK0kmUEhS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import time\n",
        "import pickle\n",
        "import csv\n",
        "import math\n",
        "import re\n",
        "import unicodedata\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "\n",
        "# Reconfigura logger\n",
        "for handler in logging.root.handlers[:]:\n",
        "    logging.root.removeHandler(handler)\n",
        "\n",
        "logging.basicConfig(\n",
        "    filename='buscador.log',\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "\n",
        "def remover_acentos(texto):\n",
        "    nfkd = unicodedata.normalize('NFKD', texto)\n",
        "    return ''.join([c for c in nfkd if not unicodedata.combining(c)])\n",
        "\n",
        "def normalizar_texto(texto):\n",
        "    texto = remover_acentos(texto)\n",
        "    texto = re.sub(r'[^A-Za-z ]+', ' ', texto)\n",
        "    return texto.upper().strip()\n",
        "\n",
        "def ler_configuracao_busca(cfg_path=\"BUSCA.CFG\"):\n",
        "    logging.info(\"Lendo BUSCA.CFG\")\n",
        "    with open('/content/BUSCA.CFG', \"r\", encoding=\"utf-8\") as f:\n",
        "        linhas = [linha.strip() for linha in f.readlines()]\n",
        "    config = {}\n",
        "    for linha in linhas:\n",
        "        chave, valor = linha.split(\"=\")\n",
        "        config[chave.strip()] = valor.strip()\n",
        "    return config[\"MODELO\"], config[\"CONSULTAS\"], config[\"RESULTADOS\"]\n",
        "\n",
        "def carregar_modelo(caminho_modelo):\n",
        "    logging.info(f\"Carregando modelo de {caminho_modelo}\")\n",
        "    with open('/content/modelo_vetorial.pkl', \"rb\") as f:\n",
        "        return pickle.load(f)\n",
        "\n",
        "def carregar_consultas(caminho_csv):\n",
        "    df = pd.read_csv('/content/consultas.csv', sep=\";\")\n",
        "    return df.to_dict(orient=\"records\")\n",
        "\n",
        "def vetorizar_consulta(texto_normalizado):\n",
        "    termos = [t for t in texto_normalizado.split() if len(t) >= 2 and t.isalpha()]\n",
        "    vetor = defaultdict(float)\n",
        "    for termo in termos:\n",
        "        vetor[termo] = 1.0  # peso fixo\n",
        "    return dict(vetor)\n",
        "\n",
        "def cosseno(v1, v2):\n",
        "    intersecao = set(v1.keys()) & set(v2.keys())\n",
        "    numerador = sum(v1[t] * v2[t] for t in intersecao)\n",
        "    denom1 = math.sqrt(sum(v**2 for v in v1.values()))\n",
        "    denom2 = math.sqrt(sum(v**2 for v in v2.values()))\n",
        "    if denom1 == 0 or denom2 == 0:\n",
        "        return 0.0\n",
        "    return numerador / (denom1 * denom2)\n",
        "\n",
        "def buscar(modelo, consultas):\n",
        "    resultados = []\n",
        "    for consulta in consultas:\n",
        "        qid = consulta[\"QueryNumber\"]\n",
        "        texto = normalizar_texto(consulta[\"QueryText\"])\n",
        "        vetor_q = vetorizar_consulta(texto)\n",
        "        ranking = []\n",
        "        for doc_id, vetor_d in modelo.items():\n",
        "            sim = cosseno(vetor_q, vetor_d)\n",
        "            if sim > 0:\n",
        "                ranking.append((doc_id, sim))\n",
        "        ranking.sort(key=lambda x: x[1], reverse=True)\n",
        "        ranking_formatado = [(i+1, doc_id, round(sim, 5)) for i, (doc_id, sim) in enumerate(ranking)]\n",
        "        resultados.append([qid, str(ranking_formatado)])\n",
        "    return resultados\n",
        "\n",
        "def salvar_resultados(resultados, caminho_saida):\n",
        "    logging.info(f\"Salvando resultados em {caminho_saida}\")\n",
        "    with open(caminho_saida, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
        "        writer = csv.writer(f, delimiter=\";\")\n",
        "        writer.writerow([\"QueryNumber\", \"Ranking\"])\n",
        "        writer.writerows(resultados)\n",
        "\n",
        "def main():\n",
        "    logging.info(\"===== Início do módulo Buscador =====\")\n",
        "    inicio = time.time()\n",
        "    try:\n",
        "        modelo_path, consultas_path, saida_path = ler_configuracao_busca()\n",
        "        modelo = carregar_modelo(modelo_path)\n",
        "        consultas = carregar_consultas(consultas_path)\n",
        "        resultados = buscar(modelo, consultas)\n",
        "        salvar_resultados(resultados, saida_path)\n",
        "        logging.info(f\"Consultas processadas: {len(consultas)}\")\n",
        "        logging.info(f\"Tempo total: {time.time() - inicio:.2f} segundos\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Erro no buscador: {e}\")\n",
        "    logging.info(\"===== Fim do módulo Buscador =====\")\n",
        "\n",
        "# Executar\n",
        "main()"
      ],
      "metadata": {
        "id": "Qd_fo2Z4T-UE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"buscador.log\", \"r\", encoding=\"utf-8\") as f:\n",
        "    print (f.read())"
      ],
      "metadata": {
        "id": "8xmuE5lTWWdx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_resultados = pd.read_csv(\"resultados.csv\", sep=\";\")\n",
        "df_resultados.head()"
      ],
      "metadata": {
        "id": "b-lkpci3csUc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ✅ Considerações Finais\n",
        "\n",
        "O sistema foi implementado com sucesso seguindo os princípios de um modelo vetorial de recuperação da informação.  \n",
        "Todos os módulos foram testados com a base Cystic Fibrosis e o formato de arquivos foi mantido conforme especificações.  \n",
        "\n",
        "**Módulos implementados**:\n",
        "- Leitura e processamento de XML com configuração\n",
        "- Lista invertida com frequência real\n",
        "- Cálculo do modelo vetorial com TF-IDF\n",
        "- Buscador com similaridade cosseno\n",
        "\n",
        "📁 Resultados disponíveis nos arquivos:\n",
        "- `consultas.csv`, `esperados.csv`\n",
        "- `lista_invertida.csv`\n",
        "- `modelo_vetorial.pkl`\n",
        "- `resultados.csv`"
      ],
      "metadata": {
        "id": "lm8uy1cKclD0"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XMhafV4ickwW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}